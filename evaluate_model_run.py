"""Ko-AgentBench ëª¨ë¸ ì‹¤í–‰ ê²°ê³¼ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

íŠ¹ì • ë‚ ì§œ/ëª¨ë¸ì˜ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ë¥¼ í‰ê°€í•˜ê³  ì¢…í•© ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
"""

import argparse
import json
import csv
import os
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime
from dotenv import load_dotenv

from bench.runner.metrics import (
    get_metrics_for_level,
    EvalContext,
    METRICS
)
from bench.adapters.litellm_adapter import LiteLLMAdapter

# Import API keys from secrets (í™˜ê²½ë³€ìˆ˜ ì„¤ì •ì€ main()ì—ì„œ)
try:
    from configs.secrets import (
        AZURE_API_KEY,
        AZURE_API_BASE,
        AZURE_API_VERSION,
        ANTHROPIC_API_KEY,
        GEMINI_API_KEY
    )
except ImportError:
    print("[ê²½ê³ ] configs.secretsë¥¼ importí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    AZURE_API_KEY = None
    AZURE_API_BASE = None
    AZURE_API_VERSION = None
    ANTHROPIC_API_KEY = None
    GEMINI_API_KEY = None


class ModelRunEvaluator:
    """ëª¨ë¸ ì‹¤í–‰ ê²°ê³¼ í‰ê°€ í´ë˜ìŠ¤"""

    # Levelë³„ íƒœìŠ¤í¬ ì˜ë„ ì„¤ëª…
    LEVEL_DESCRIPTIONS = {
        '1': 'ê°€ì¥ ê¸°ë³¸ì ì¸ API í˜¸ì¶œ ëŠ¥ë ¥ ê²€ì¦. ì£¼ì–´ì§„ ë‹¨ì¼ ë„êµ¬ë¥¼ ì •í™•í•œ íŒŒë¼ë¯¸í„°ë¡œ í˜¸ì¶œí•  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸',
        '2': 'ì—¬ëŸ¬ í›„ë³´ ë„êµ¬ ì¤‘ ìµœì ì˜ APIë¥¼ ì„ íƒí•˜ëŠ” ëŠ¥ë ¥ ê²€ì¦. ì£¼ì–´ì§„ ë„êµ¬ ëª©ë¡ ì¤‘ ê°€ì¥ ì í•©í•œ ë„êµ¬ë¥¼ ì„ íƒí•  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸',
        '3': 'ì—¬ëŸ¬ ë„êµ¬ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ í˜¸ì¶œí•˜ê³ , í•œ ë„êµ¬ì˜ ê²°ê³¼ë¥¼ ë‹¤ìŒ ë„êµ¬ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ë³µì¡í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ëŠ¥ë ¥ ê²€ì¦',
        '4': 'ì—¬ëŸ¬ ë„êµ¬ë¥¼ ë³‘ë ¬ì ìœ¼ë¡œ í˜¸ì¶œí•˜ì—¬ ì–»ì€ ì •ë³´ë¥¼ ì¢…í•©í•˜ê³ , ë¹„êµÂ·ë¶„ì„í•˜ì—¬ ìµœì¢… ê²°ë¡ ì„ ë„ì¶œí•˜ëŠ” ëŠ¥ë ¥ ê²€ì¦',
        '5': 'ì •ë³´ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ API í˜¸ì¶œì´ ì‹¤íŒ¨í•˜ëŠ” ë“± ì˜ˆì™¸ì ì¸ ìƒí™©ì— ëŒ€ì²˜í•˜ëŠ” ëŠ¥ë ¥ ê²€ì¦',
        '6': 'ì´ì „ ëŒ€í™”ì—ì„œ ì–»ì€ Tool í˜¸ì¶œ ê²°ê³¼ë¥¼ ê¸°ì–µí•˜ê³ , ë¶ˆí•„ìš”í•œ API ì¬í˜¸ì¶œ ì—†ì´ íš¨ìœ¨ì ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” ëŠ¥ë ¥ ê²€ì¦',
        '7': 'ì—¬ëŸ¬ í„´ì— ê±¸ì¹œ ëŒ€í™”ì˜ í•µì‹¬ ë§¥ë½ì„ ê¸°ì–µí•˜ê³ , ì´ë¥¼ ìƒˆë¡œìš´ ì§ˆë¬¸ê³¼ ì—°ê²°í•˜ì—¬ ì •í™•í•œ Tool callingì„ ìˆ˜í–‰í•˜ëŠ” ëŠ¥ë ¥ ê²€ì¦'
    }

    # Metricë³„ ìƒì„¸ ì„¤ëª…
    METRIC_DESCRIPTIONS = {
        'RRR': 'ì‘ë‹µ ë°˜í™˜ìœ¨. ëª¨ë¸ì´ ê¸°ìˆ ì  ì˜¤ë¥˜(Exception, Timeout ë“±) ì—†ì´ ìµœì¢… ì‘ë‹µì„ ë°˜í™˜í–ˆëŠ”ì§€ ì—¬ë¶€',
        'ArgAcc': 'ì¸ì ì •í™•ë„. ë„êµ¬ í˜¸ì¶œ ì‹œ ì „ë‹¬í•œ ì¸ìë“¤ì´ ì •ë‹µê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í‰ê°€',
        'CallEM': 'í˜¸ì¶œ ì™„ì „ ì¼ì¹˜. ë„êµ¬ëª…ê³¼ ëª¨ë“  ì¸ìê°€ ì •ë‹µê³¼ ì™„ë²½í•˜ê²Œ ì¼ì¹˜í•˜ëŠ”ì§€ í‰ê°€',
        'EPR_CVR': 'ìœ íš¨ í˜¸ì¶œ ë¹„ìœ¨. ìƒì„±í•œ ë„êµ¬ í˜¸ì¶œì´ ìŠ¤í‚¤ë§ˆìƒ ìœ íš¨í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•œì§€ í‰ê°€',
        'RespOK': 'ì‘ë‹µ íŒŒì‹± ì„±ê³µ. ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ ì„±ê³µì ìœ¼ë¡œ íŒŒì‹±í–ˆëŠ”ì§€ í‰ê°€',
        'SR': 'ì„±ê³µë¥ . ì£¼ì–´ì§„ íƒœìŠ¤í¬ë¥¼ ìµœì¢…ì ìœ¼ë¡œ ì„±ê³µí–ˆëŠ”ì§€ ì—¬ë¶€',
        'ToolAcc': 'ë„êµ¬ ì„ íƒ ì •í™•ë„. ì •ë‹µ ë„êµ¬ë¥¼ ì •í™•í•˜ê²Œ ì„ íƒí–ˆëŠ”ì§€ í‰ê°€',
        'pass@k': 'ë°˜ë³µ ì•ˆì •ì„±. íƒœìŠ¤í¬ë¥¼ kë²ˆ ë°˜ë³µ ìˆ˜í–‰í–ˆì„ ë•Œ ìµœì†Œ í•œ ë²ˆ ì´ìƒ ì„±ê³µí•˜ëŠ” ë¹„ìœ¨',
        'SelectAcc': 'ìµœì¢… ì„ íƒ ë„êµ¬ ì •í™•ë„. ì—¬ëŸ¬ í›„ë³´êµ° ì¤‘ì—ì„œ ìµœì¢…ì ìœ¼ë¡œ ì˜¬ë°”ë¥¸ ë„êµ¬ë¥¼ ì„ íƒí–ˆëŠ”ì§€ í‰ê°€',
        'FSM': 'ì •ë‹µ ê²½ë¡œ ì™„ì „ ì¼ì¹˜. ì •í•´ì§„ ë„êµ¬ í˜¸ì¶œ ìˆœì„œì™€ ì™„ë²½í•˜ê²Œ ì¼ì¹˜í•˜ëŠ”ì§€ í‰ê°€',
        'PSM': 'ì •ë‹µ ê²½ë¡œ ë¶€ë¶„ ì¼ì¹˜. ì •ë‹µ ê²½ë¡œì˜ ì¼ë¶€ë¥¼ ì–¼ë§ˆë‚˜ í¬í•¨í•˜ê³  ìˆëŠ”ì§€ í‰ê°€',
        'ProvAcc': 'ë°ì´í„° ì¶œì²˜ ì¶”ì  ì •í™•ë„. ì´ì „ ë‹¨ê³„ì˜ ì¶œë ¥ê°’ì„ ë‹¤ìŒ ë‹¨ê³„ì˜ ì…ë ¥ê°’ìœ¼ë¡œ ì •í™•íˆ ì—°ê²°í–ˆëŠ”ì§€ í‰ê°€',
        'Î”Steps_norm': 'ìµœì†Œ ê²½ë¡œ ëŒ€ë¹„ íš¨ìœ¨. ì´ë¡ ì ì¸ ìµœì†Œ í˜¸ì¶œ íšŸìˆ˜ ëŒ€ë¹„ ì–¼ë§ˆë‚˜ íš¨ìœ¨ì ì¸ ê²½ë¡œë¥¼ ìƒì„±í–ˆëŠ”ì§€ í‰ê°€',
        'Coverage': 'ì†ŒìŠ¤ ì»¤ë²„ë¦¬ì§€. ì •ë³´ë¥¼ ìˆ˜ì§‘í•´ì•¼ í•˜ëŠ” ì—¬ëŸ¬ ì†ŒìŠ¤ë¥¼ ëˆ„ë½ ì—†ì´ í˜¸ì¶œí–ˆëŠ”ì§€ í‰ê°€',
        'SourceEPR': 'ì†ŒìŠ¤ë³„ ìœ íš¨ í˜¸ì¶œ ë¹„ìœ¨. ë³‘ë ¬ì ìœ¼ë¡œ í˜¸ì¶œí•œ ê° ë„êµ¬ê°€ ìœ íš¨í–ˆëŠ”ì§€ ê°œë³„ì ìœ¼ë¡œ í‰ê°€',
        'AdaptiveRoutingScore': 'ì ì‘í˜• ë¼ìš°íŒ… ì ìˆ˜. ì£¼ì…ëœ ë„êµ¬ ì‹¤íŒ¨ ì´í›„ ì–¼ë§ˆë‚˜ ì‹ ì†í•˜ê²Œ ëŒ€ì²´ ê²½ë¡œë¡œ ì „í™˜í•˜ëŠ”ì§€ í‰ê°€',
        'FallbackSR': 'ëŒ€ì²´ ê²½ë¡œ ì„±ê³µë¥ . íŠ¹ì • ë„êµ¬ ì‹¤íŒ¨ ì‹œ ë‹¤ë¥¸ ë„êµ¬ë¥¼ í™œìš©í•´ ì„±ê³µí•˜ëŠ” ë¹„ìœ¨',
        'EffScore': 'íš¨ìœ¨ ì ìˆ˜. ì´ë¡ ì  ìµœì†Œ í˜¸ì¶œ ìˆ˜ì™€ ì¬ì‚¬ìš©ë¥ ì„ ì¢…í•©í•˜ì—¬ íš¨ìœ¨ì„±ì„ ì ìˆ˜í™”',
        'RedundantCallRate': 'ë¶ˆí•„ìš” í˜¸ì¶œ ë¹„ìœ¨. ì •ë³´ë¥¼ ì´ë¯¸ ì•Œê³  ìˆìŒì—ë„ ë¶ˆí•„ìš”í•˜ê²Œ ë„êµ¬ë¥¼ ë‹¤ì‹œ í˜¸ì¶œí•˜ëŠ” ë¹„ìœ¨',
        'ReuseRate': 'ì¬ì‚¬ìš© ë¹„ìœ¨. ì´ì „ì— í˜¸ì¶œí–ˆë˜ ê²°ê³¼ë¥¼ ì¬í˜¸ì¶œ ì—†ì´ íš¨ìœ¨ì ìœ¼ë¡œ ì¬ì‚¬ìš©í•˜ëŠ” ë¹„ìœ¨',
        'ContextRetention': 'ë§¥ë½ ìœ ì§€ìœ¨. ì—¬ëŸ¬ í„´ì— ê±¸ì¹œ ëŒ€í™”ì˜ í•µì‹¬ ë§¥ë½ì„ ë‹µë³€ì— ì˜¬ë°”ë¥´ê²Œ ìœ ì§€í•˜ëŠ”ì§€ í‰ê°€',
        'RefRecall': 'ì¥ê¸° íšŒìƒ ë¹„ìœ¨. ëŒ€í™” ì´ˆë°˜ì˜ ì •ë³´ë¥¼ ë§ˆì§€ë§‰ í„´ì—ì„œ ë‹¤ì‹œ ì§ˆë¬¸í–ˆì„ ë•Œ ì •í™•íˆ ê¸°ì–µí•´ë‚´ëŠ”ì§€ í‰ê°€'
    }

    def __init__(
            self,
            date: str,
            model: str,
            judge_models: List[str],
            sample_size: Optional[int] = None,
            levels: Optional[List[str]] = None,
            verbose: bool = False
    ):
        self.date = date
        self.model = model
        self.judge_models = judge_models
        self.sample_size = sample_size
        self.levels = levels  # í‰ê°€í•  ë ˆë²¨ ëª©ë¡
        self.verbose = verbose
        self.judge_adapters = []

        # LLM Judge adapter ì´ˆê¸°í™”
        print(f"[ì´ˆê¸°í™”] Judge ëª¨ë¸ ë¡œë”©: {', '.join(self.judge_models)}")

        for judge_model in self.judge_models:
            try:
                adapter = LiteLLMAdapter(judge_model)
                self.judge_adapters.append(adapter)
                print(f"{judge_model} ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"{judge_model} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

        if not self.judge_adapters:
            print(f"[ê²½ê³ ] Judge ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨")
        else:
            self._inject_judge_to_metrics()
            print(f"[OK] {len(self.judge_adapters)}ê°œ Judge ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")

    def _inject_judge_to_metrics(self):
        """Judge ëª¨ë¸ì„ í•„ìš”í•œ ë©”íŠ¸ë¦­ì— ì£¼ì…"""
        if not self.judge_adapters:
            return

        judge_metrics = [
            'SR',               # ê³µí†µ
            'ArgAcc',           # L1
            'EffScore',         # L6
            'ContextRetention', # L7
            'RefRecall'         # L7
        ]

        for metric_name in judge_metrics:
            if metric_name in METRICS:
                if hasattr(METRICS[metric_name], '__dict__'):
                    METRICS[metric_name].llm_adapters = self.judge_adapters
                    METRICS[metric_name].llm_adapter = self.judge_adapters[0]



    def find_result_files(self) -> Dict[str, Path]:
        """ë‚ ì§œì™€ ëª¨ë¸ë¡œ L1~L7 íŒŒì¼ ì°¾ê¸°"""
        results_dir = Path('logs/benchmark_results')

        # ëª¨ë¸ëª…ì„ íŒŒì¼ëª… íŒ¨í„´ìœ¼ë¡œ ë³€í™˜ (azure/gpt-4.1 -> azure_gpt-4.1)
        model_pattern = self.model.replace('/', '_')

        level_files = {}

        # ê¸°ì¡´ íŒ¨í„´: logs/benchmark_results/L{level}_{model}_{date}*.json)
        pattern = f"L*_{model_pattern}_{self.date}*.json"
        files = list(results_dir.glob(pattern))

        for f in files:
            level = f.name.split('_')[0]
            if level.startswith('L') and level[1:].isdigit():
                level_files[level] = f

        # ë³€ê²½ íŒ¨í„´: logs/benchmark_results/by_model/{model}/{date_timestamp}/L*.json)
        if not level_files:
            by_model_dir = results_dir / 'by_model' / model_pattern

            if by_model_dir.exists():
                date_dirs = [d for d in by_model_dir.iterdir()
                             if d.is_dir() and d.name.startswith(self.date)]

                for level_name in ['L1', 'L2', 'L3', 'L4', 'L5', 'L6', 'L7']:
                    for date_dir in date_dirs:
                        level_file = date_dir / f"{level_name}.json"
                        if level_file.exists() and level_name not in level_files:
                            level_files[level_name] = level_file
                            break

        return level_files

    def evaluate_level(self, file_path: Path, level: str) -> Dict[str, Any]:
        """íŠ¹ì • ë ˆë²¨ì˜ ê²°ê³¼ íŒŒì¼ í‰ê°€"""
        level_num = int(level[1])  # "L1" -> 1

        # ê²°ê³¼ íŒŒì¼ ë¡œë“œ
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        metadata = data.get("metadata", {})
        tasks = data.get("results", [])

        if self.verbose:
            print(f"  íŒŒì¼: {file_path.name}")
            print(f"  ì „ì²´ íƒœìŠ¤í¬: {len(tasks)}")

        if not tasks:
            print(f"  [ê²½ê³ ] í‰ê°€í•  íƒœìŠ¤í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {
                "level": level_num,
                "file": file_path.name,
                "metadata": metadata,
                "total_tasks": 0,
                "evaluated_tasks": 0,
                "metric_averages": {},
                "task_evaluations": []
            }

        # ìƒ˜í”Œë§
        if self.sample_size:
            tasks_to_evaluate = tasks[:self.sample_size]
            if self.verbose:
                print(f"  ìƒ˜í”Œë§: {len(tasks_to_evaluate)}ê°œ íƒœìŠ¤í¬")
        else:
            tasks_to_evaluate = tasks

        # í•´ë‹¹ ë ˆë²¨ì˜ ë©”íŠ¸ë¦­ ê°€ì ¸ì˜¤ê¸°
        metrics = get_metrics_for_level(level_num)

        # ê° íƒœìŠ¤í¬ í‰ê°€
        all_evaluations = []

        for idx, task_result in enumerate(tasks_to_evaluate, 1):
            task_id = task_result.get("task_id", f"unknown_{idx}")

            if self.verbose:
                print(f"  [{idx}/{len(tasks_to_evaluate)}] {task_id}")

            # task_schema êµ¬ì„± 
            task_schema = {
                "task_id": task_result.get("task_id"),
                "instruction": task_result.get("instruction"),
                "task_level": task_result.get("level"),
                "task_category": task_result.get("category"),
                "golden_action": task_result.get("golden_action", []),
                "minimum_steps": task_result.get("minimum_steps"),
                "data_flow": task_result.get("data_flow", []),
                "available_tools": task_result.get("expected_tools", []),
                "error_injection": task_result.get("error_injection"),
                "fallback_options": task_result.get("fallback_options", []),
                "resp_schema": task_result.get("resp_schema"),
                "arg_schema": task_result.get("arg_schema"),
                "repetitions": task_result.get("repetitions", 1),  # pass@kìš©
            }

            # logs êµ¬ì„±
            logs = {
                "success": task_result.get("success", False),
                "tool_invocations": task_result.get("tool_calls", []),
                "tool_calls": task_result.get("tool_calls", []),  # AdaptiveRoutingScore/FallbackSR í‰ê°€ìš©
                "actual_output": task_result.get("final_response", ""),
                "final_response": task_result.get("final_response", ""),
                "conversation_log": task_result.get("conversation_log", {}),
                "repetition_results": task_result.get("repetition_results", []),  # pass@kìš©
            }

            # EvalContext ìƒì„± ë° ë©”íŠ¸ë¦­ í‰ê°€
            try:
                ctx = EvalContext(task_schema=task_schema, logs=logs)

                task_evaluation = {
                    "task_id": task_id,
                    "success": task_result.get("success", False),
                    "metrics": {}
                }

                for metric_name, metric in metrics.items():
                    try:
                        result = metric.evaluate(ctx)
                        task_evaluation["metrics"][metric_name] = {
                            "score": result.score,
                            "details": result.details
                        }
                    except Exception as e:
                        if self.verbose:
                            print(f"    [ì˜¤ë¥˜] {metric_name}: {e}")
                        task_evaluation["metrics"][metric_name] = {
                            "score": 0.0,
                            "error": str(e)
                        }

                all_evaluations.append(task_evaluation)

            except Exception as e:
                print(f"  [ì˜¤ë¥˜] {task_id} í‰ê°€ ì‹¤íŒ¨: {e}")
                continue

        # RRR (Response Return Rate) ê³„ì‚° ë¡œì§ ì¶”ê°€
        # ê¸°ìˆ ì  ì˜¤ë¥˜ ì—†ì´ ì‘ë‹µì„ ë°˜í™˜í•œ ë¹„ìœ¨
        successful_responses = sum(1 for task in all_evaluations if task.get("success", False))
        rrr_score = successful_responses / len(all_evaluations) if all_evaluations else 0.0

        # ë©”íŠ¸ë¦­ë³„ í‰ê·  ê³„ì‚°
        metric_averages = {}
        for metric_name in metrics.keys():
            scores = [
                task["metrics"].get(metric_name, {}).get("score", 0.0)
                for task in all_evaluations
                if metric_name in task.get("metrics", {})
            ]
            # None ê°’ í•„í„°ë§
            valid_scores = [score for score in scores if score is not None]
            if valid_scores:
                metric_averages[metric_name] = sum(valid_scores) / len(valid_scores)
            else:
                metric_averages[metric_name] = 0.0
        metric_averages["RRR"] = rrr_score

        return {
            "level": level_num,
            "file": file_path.name,
            "metadata": metadata,
            "total_tasks": len(tasks),
            "evaluated_tasks": len(all_evaluations),
            "metric_averages": metric_averages,
            "task_evaluations": all_evaluations
        }

    def evaluate(self) -> Dict[str, Any]:
        """ì „ì²´ í‰ê°€ ì‹¤í–‰"""
        print(f"\n{'='*80}")
        print(f"í‰ê°€ ì‹œì‘")
        print(f"{'='*80}")
        print(f"ëª¨ë¸: {self.model}")
        print(f"ë‚ ì§œ: {self.date}")
        if self.levels:
            print(f"í‰ê°€ ë ˆë²¨: {', '.join(self.levels)}")
        if self.sample_size:
            print(f"ìƒ˜í”Œë§: ë ˆë²¨ë‹¹ {self.sample_size}ê°œ")
        print(f"{'='*80}\n")

        # ê²°ê³¼ íŒŒì¼ ì°¾ê¸°
        level_files = self.find_result_files()

        if not level_files:
            raise FileNotFoundError(
                f"ë‚ ì§œ {self.date}, ëª¨ë¸ {self.model}ì— í•´ë‹¹í•˜ëŠ” ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )

        # í‰ê°€í•  ë ˆë²¨ í•„í„°ë§
        if self.levels:
            # ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ì²˜ë¦¬ (L1, l1 ëª¨ë‘ í—ˆìš©)
            requested_levels = [lvl.upper() if not lvl.startswith('L') else lvl for lvl in self.levels]
            requested_levels = ['L' + lvl if not lvl.startswith('L') else lvl for lvl in requested_levels]
            
            filtered_files = {k: v for k, v in level_files.items() if k in requested_levels}
            
            if not filtered_files:
                available = ', '.join(sorted(level_files.keys()))
                requested = ', '.join(requested_levels)
                raise ValueError(
                    f"ìš”ì²­í•œ ë ˆë²¨({requested})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
                    f"ì‚¬ìš© ê°€ëŠ¥í•œ ë ˆë²¨: {available}"
                )
            level_files = filtered_files

        print(f"ë°œê²¬ëœ íŒŒì¼:")
        for level, path in sorted(level_files.items()):
            print(f"  {level}: {path.name}")
        print()

        # ê° ë ˆë²¨ í‰ê°€
        by_level = {}
        total_tasks = 0
        evaluated_tasks = 0

        for level in sorted(level_files.keys()):
            print(f"[{level}] í‰ê°€ ì¤‘...")
            level_result = self.evaluate_level(level_files[level], level)

            by_level[level] = {
                "file": level_result["file"],
                "total_tasks": level_result["total_tasks"],
                "evaluated_tasks": level_result["evaluated_tasks"],
                "metrics": level_result["metric_averages"],
                "metadata": level_result["metadata"],
                "task_evaluations": level_result["task_evaluations"]
            }

            total_tasks += level_result["total_tasks"]
            evaluated_tasks += level_result["evaluated_tasks"]

            print(f"  ì™„ë£Œ: {level_result['evaluated_tasks']}ê°œ íƒœìŠ¤í¬ í‰ê°€\n")

        # ì¢…í•© ë¦¬í¬íŠ¸
        report = {
            "summary": {
                "model": self.model,
                "judge_model": ", ".join(self.judge_models),
                "execution_date": self.date,
                "evaluation_date": datetime.now().isoformat(),
                "total_tasks": total_tasks,
                "evaluated_tasks": evaluated_tasks,
                "sample_size": self.sample_size,
                "levels_evaluated": len(by_level)
            },
            "by_level": by_level
        }

        print(f"{'='*80}")
        print(f"í‰ê°€ ì™„ë£Œ")
        print(f"{'='*80}")
        print(f"ì´ ë ˆë²¨: {len(by_level)}")
        print(f"ì´ íƒœìŠ¤í¬: {evaluated_tasks}/{total_tasks}")
        print(f"{'='*80}\n")

        return report

    def export_json(self, report: Dict[str, Any], output_dir: Path):
        """JSON ë¦¬í¬íŠ¸ ìƒì„±"""
        output_file = output_dir / "evaluation_report.json"

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"[ì €ì¥] JSON: {output_file}")

    def export_csv(self, report: Dict[str, Any], output_dir: Path):
        """CSV ë¦¬í¬íŠ¸ ìƒì„±"""
        output_file = output_dir / "evaluation_summary.csv"

        # ëª¨ë“  ë©”íŠ¸ë¦­ ì´ë¦„ ìˆ˜ì§‘
        all_metrics = set()
        for level_data in report['by_level'].values():
            all_metrics.update(level_data['metrics'].keys())

        # CSV ì‘ì„±
        with open(output_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)

            # í—¤ë”
            header = ['Level', 'Total_Tasks', 'Evaluated_Tasks', 'Avg_Exec_Time', 'Avg_Tokens', 'Avg_TPS', 'Avg_TTFT'] + sorted(all_metrics)
            writer.writerow(header)

            # ê° ë ˆë²¨ ë°ì´í„°
            for level in sorted(report['by_level'].keys()):
                level_data = report['by_level'][level]
                metadata = level_data.get('metadata', {})
                
                row = [
                    level,
                    level_data['total_tasks'],
                    level_data['evaluated_tasks'],
                    f"{metadata.get('average_execution_time', 0):.2f}",
                    f"{metadata.get('average_tokens_per_task', 0):.2f}",
                    f"{metadata.get('average_tps', 0):.2f}",
                    f"{metadata.get('ttft', {}).get('average', 0):.4f}",
                ]
                for metric in sorted(all_metrics):
                    score = level_data['metrics'].get(metric, 0.0)
                    row.append(f"{score:.4f}")
                writer.writerow(row)

        print(f"[ì €ì¥] CSV: {output_file}")

    def export_markdown(self, report: Dict[str, Any], output_dir: Path):
        """Markdown ë¦¬í¬íŠ¸ ìƒì„±"""
        output_file = output_dir / "evaluation_report.md"

        with open(output_file, 'w', encoding='utf-8') as f:
            # í—¤ë”
            f.write("# Ko-AgentBench í‰ê°€ ë³´ê³ ì„œ\n\n")

            # ì‹¤í–‰ ì •ë³´
            summary = report['summary']
            f.write("## ì‹¤í–‰ ì •ë³´\n\n")
            f.write(f"- **í‰ê°€ ëŒ€ìƒ ëª¨ë¸**: {summary['model']}\n")
            f.write(f"- **Judge ëª¨ë¸**: {summary['judge_model']}\n")
            f.write(f"- **ì‹¤í–‰ ë‚ ì§œ**: {summary['execution_date']}\n")
            f.write(f"- **í‰ê°€ ë‚ ì§œ**: {summary['evaluation_date'][:10]}\n")
            f.write(f"- **ì´ íƒœìŠ¤í¬**: {summary['evaluated_tasks']}/{summary['total_tasks']}\n")
            if summary['sample_size']:
                f.write(f"- **ìƒ˜í”Œë§**: ë ˆë²¨ë‹¹ {summary['sample_size']}ê°œ\n")
            f.write("\n")

            # ğŸ“Š ì„±ëŠ¥ ìš”ì•½ í…Œì´ë¸” ì¶”ê°€
            f.write("## ğŸ“Š ì„±ëŠ¥ ìš”ì•½\n\n")
            f.write("| Level | íƒœìŠ¤í¬ ìˆ˜ | í‰ê·  ì‹¤í–‰ì‹œê°„ | í‰ê·  TPS | í‰ê·  TTFT | ì£¼ìš” ì§€í‘œ |\n")
            f.write("| --- | --- | --- | --- | --- | --- |\n")

            # ê° ë ˆë²¨ì˜ í•µì‹¬ ë©”íŠ¸ë¦­ ë§¤í•‘
            key_metrics = {
                'L1': ['ToolAcc', 'ArgAcc'],
                'L2': ['SelectAcc'],
                'L3': ['FSM', 'PSM'],
                'L4': ['Coverage'],
                'L5': ['AdaptiveRoutingScore', 'FallbackSR'],
                'L6': ['EffScore', 'ReuseRate'],
                'L7': ['ContextRetention']
            }

            for level in sorted(report['by_level'].keys()):
                level_data = report['by_level'][level]
                metadata = level_data.get('metadata', {})
                metrics = level_data['metrics']

                task_count = f"{level_data['evaluated_tasks']}/{level_data['total_tasks']}"
                
                exec_time = f"{metadata.get('average_execution_time', 0):.1f}ì´ˆ" if 'average_execution_time' in metadata else "N/A"
                tps = f"{metadata.get('average_tps', 0):.0f}" if 'average_tps' in metadata else "N/A"
                ttft = f"{metadata.get('ttft', {}).get('average', 0):.3f}ì´ˆ" if 'ttft' in metadata else "N/A"

                # í•µì‹¬ ë©”íŠ¸ë¦­ í‘œì‹œ
                key_metric_strs = []
                for km in key_metrics.get(level, []):
                    if km in metrics:
                        key_metric_strs.append(f"{km}: {metrics[km]:.3f}")
                key_metric_str = ", ".join(key_metric_strs) if key_metric_strs else "N/A"

                f.write(f"| **{level}** | {task_count} | {exec_time} | {tps} | {ttft} | {key_metric_str} |\n")
            f.write("\n")

            # ë ˆë²¨ë³„ ìƒì„¸ ì„±ëŠ¥
            f.write("## ë ˆë²¨ë³„ ì„±ëŠ¥\n\n")

            for level in sorted(report['by_level'].keys()):
                level_data = report['by_level'][level]
                level_num = level[1]

                level_names = {
                    '1': 'Level 1: ë‹¨ì¼ ë„êµ¬ í˜¸ì¶œ',
                    '2': 'Level 2: ë„êµ¬ ì„ íƒ',
                    '3': 'Level 3: ë©€í‹°ìŠ¤í… ì¶”ë¡ ',
                    '4': 'Level 4: ë©€í‹°ì†ŒìŠ¤ í†µí•©',
                    '5': 'Level 5: ì˜¤ë¥˜ ì²˜ë¦¬',
                    '6': 'Level 6: ì»¨í…ìŠ¤íŠ¸ ì¬ì‚¬ìš©',
                    '7': 'Level 7: ë©€í‹°í„´ ëŒ€í™”',
                }

                f.write(f"### {level_names.get(level_num, level)}\n\n")

                # íƒœìŠ¤í¬ ì˜ë„ ì„¤ëª… ì¶”ê°€
                if level_num in self.LEVEL_DESCRIPTIONS:
                    f.write(f"> íƒœìŠ¤í¬ ì˜ë„: {self.LEVEL_DESCRIPTIONS[level_num]}\n>\n")

                f.write(f"- íƒœìŠ¤í¬ ìˆ˜: {level_data['evaluated_tasks']}/{level_data['total_tasks']}\n")

                metadata = level_data.get('metadata', {})
                if 'average_execution_time' in metadata:
                    f.write(f"- í‰ê·  ì‹¤í–‰ì‹œê°„: {metadata['average_execution_time']:.2f}ì´ˆ\n")

                f.write("\n**ë©”íŠ¸ë¦­ ì ìˆ˜:**\n\n")

                metrics = level_data['metrics']
                if metrics:
                    # RRRê³¼ SRì„ ë¨¼ì € ì¶œë ¥
                    priority_metrics = ['RRR', 'SR']
                    for metric_name in priority_metrics:
                        if metric_name in metrics:
                            score = metrics[metric_name]
                            description = self.METRIC_DESCRIPTIONS.get(metric_name, "ì„¤ëª… ì—†ìŒ")
                            f.write(f"- **{metric_name}**: {score:.3f} - {description}\n")
                    
                    # ë‚˜ë¨¸ì§€ ë©”íŠ¸ë¦­ ì¶œë ¥
                    for metric_name in sorted(metrics.keys()):
                        if metric_name not in priority_metrics:
                            score = metrics[metric_name]
                            description = self.METRIC_DESCRIPTIONS.get(metric_name, "ì„¤ëª… ì—†ìŒ")
                            f.write(f"- **{metric_name}**: {score:.3f} - {description}\n")
                else:
                    f.write("- (ë©”íŠ¸ë¦­ ì—†ìŒ)\n")

                f.write("\n")

            # í† í° ì‚¬ìš©ëŸ‰ ë° ì„±ëŠ¥ ì§€í‘œ ì„¹ì…˜
            f.write("## í† í° ì‚¬ìš©ëŸ‰ ë° ì„±ëŠ¥ ì§€í‘œ\n\n")
            
            total_tokens = 0
            total_time = 0
            total_tasks = 0
            all_ttft_values = []
            
            for level_data in report['by_level'].values():
                metadata = level_data.get('metadata', {})
                tasks = level_data.get('evaluated_tasks', 0)
                total_tasks += tasks
                total_tokens += metadata.get('total_tokens', 0)
                total_time += metadata.get('total_execution_time', 0)
                
                ttft_avg = metadata.get('ttft', {}).get('average', 0)
                if ttft_avg > 0:
                    all_ttft_values.extend([ttft_avg] * tasks)
            
            # ì „ì²´ í‰ê·  ê³„ì‚°
            overall_tps = total_tokens / total_time if total_time > 0 else 0
            overall_avg_ttft = sum(all_ttft_values) / len(all_ttft_values) if all_ttft_values else 0
            
            f.write("### ì „ì²´ ìš”ì•½\n\n")
            f.write(f"- **ì´ ì²˜ë¦¬ í† í°**: {total_tokens:,}ê°œ\n")
            f.write(f"- **ì´ ì‹¤í–‰ ì‹œê°„**: {total_time:.2f}ì´ˆ\n")
            f.write(f"- **ì „ì²´ í‰ê·  TPS**: {overall_tps:.2f} tokens/sec\n")
            f.write(f"- **ì „ì²´ í‰ê·  TTFT**: {overall_avg_ttft:.4f}ì´ˆ\n\n")
            
            f.write("### ë ˆë²¨ë³„ ìƒì„¸\n\n")
            f.write("| Level | í‰ê·  í† í° ìˆ˜ | ì…ë ¥ í† í° | ì¶œë ¥ í† í° | TPS | TTFT (í‰ê· ) | TTFT (ìµœì†Œ/ìµœëŒ€) |\n")
            f.write("| --- | --- | --- | --- | --- | --- | --- |\n")
            
            for level in sorted(report['by_level'].keys()):
                level_data = report['by_level'][level]
                metadata = level_data.get('metadata', {})
                
                avg_tokens = metadata.get('average_tokens_per_task', 0)
                avg_prompt = metadata.get('average_prompt_tokens', 0)
                avg_completion = metadata.get('average_completion_tokens', 0)
                tps = metadata.get('average_tps', 0)
                
                ttft_data = metadata.get('ttft', {})
                ttft_avg = ttft_data.get('average', 0)
                ttft_min = ttft_data.get('min', 0)
                ttft_max = ttft_data.get('max', 0)
                ttft_range = f"{ttft_min:.4f} / {ttft_max:.4f}" if ttft_min > 0 else "N/A"
                
                f.write(f"| **{level}** | {avg_tokens:.0f} | {avg_prompt:.0f} | {avg_completion:.0f} | "
                    f"{tps:.0f} | {ttft_avg:.4f}ì´ˆ | {ttft_range} |\n")
            
            f.write("\n")
            
        print(f"[ì €ì¥] Markdown: {output_file}")

    def export(self, report: Dict[str, Any], output_dir: str, formats: List[str]):
        """ê²°ê³¼ ë‚´ë³´ë‚´ê¸°"""
        output_path = Path(output_dir)

        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        model_safe = self.model.replace('/', '_')
        final_output_dir = output_path / f"{model_safe}_{self.date}"
        final_output_dir.mkdir(parents=True, exist_ok=True)

        print(f"\n{'='*80}")
        print(f"ê²°ê³¼ ì €ì¥")
        print(f"{'='*80}")

        # í˜•ì‹ë³„ ì €ì¥
        if 'all' in formats:
            formats = ['json', 'csv', 'markdown']

        if 'json' in formats:
            self.export_json(report, final_output_dir)

        if 'csv' in formats:
            self.export_csv(report, final_output_dir)

        if 'markdown' in formats:
            self.export_markdown(report, final_output_dir)

        print(f"{'='*80}\n")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    load_dotenv()

    # Set API keys from secrets.py to environment variables
    if AZURE_API_KEY:
        os.environ['AZURE_API_KEY'] = AZURE_API_KEY
    if AZURE_API_BASE:
        os.environ['AZURE_API_BASE'] = AZURE_API_BASE
    if AZURE_API_VERSION:
        os.environ['AZURE_API_VERSION'] = AZURE_API_VERSION
    if ANTHROPIC_API_KEY:
        os.environ['ANTHROPIC_API_KEY'] = ANTHROPIC_API_KEY
    if GEMINI_API_KEY:
        os.environ['GEMINI_API_KEY'] = GEMINI_API_KEY

    parser = argparse.ArgumentParser(
        description='Ko-AgentBench ëª¨ë¸ ì‹¤í–‰ ê²°ê³¼ í‰ê°€',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # ê¸°ë³¸ ì‚¬ìš© (ëª¨ë“  ë ˆë²¨, ë‹¨ì¼ Judge )
  python evaluate_model_run.py --date 20251016 --model azure/gpt-4.1
  
  # íŠ¹ì • ë ˆë²¨ë§Œ í‰ê°€
  python evaluate_model_run.py --date 20251021 --model anthropic/claude-sonnet-4-5 --levels L1
  python evaluate_model_run.py --date 20251021 --model anthropic/claude-sonnet-4-5 --levels L1,L3,L5
  
  # Judge ëª¨ë¸ ì§€ì •
  python evaluate_model_run.py --date 20251016 --model azure/gpt-4.1 --judge-models azure/gpt-4o gemini/gemini-2.5-pro-preview-03-25
  
  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (ê° ë ˆë²¨ë‹¹ 1ê°œ)
  python evaluate_model_run.py --date 20251016 --model azure/gpt-4.1 --quick
  
  # ìƒ˜í”Œë§
  python evaluate_model_run.py --date 20251016 --model azure/gpt-4.1 --sample 10
        """
    )

    # í•„ìˆ˜ íŒŒë¼ë¯¸í„°
    parser.add_argument('--date', required=True,
                        help='ì‹¤í–‰ ë‚ ì§œ (ì˜ˆ: 20251016)')
    parser.add_argument('--model', required=True,
                        help='í‰ê°€ ëŒ€ìƒ ëª¨ë¸ (ì˜ˆ: azure/gpt-4.1)')

    # í‰ê°€ ì„¤ì •
    parser.add_argument('--levels', type=str, default=None,
                        help='í‰ê°€í•  ë ˆë²¨ ì§€ì • (ì˜ˆ: L1, L1,L3,L5). ê¸°ë³¸: ëª¨ë“  ë ˆë²¨')
    parser.add_argument('--judge-models', nargs='+', default=None,
                        help='LLM Judge ëª¨ë¸(ë“¤). ê¸°ë³¸: gpt-4o ë‹¨ì¼ ëª¨ë¸. ì•™ìƒë¸” ì›í•˜ë©´ ì—¬ëŸ¬ ê°œ ì§€ì • (ì˜ˆ: gpt-4o claude-sonnet-4-5 gemini-2.5)')
    parser.add_argument('--sample', type=int, default=None,
                        help='ë ˆë²¨ë‹¹ í‰ê°€í•  íƒœìŠ¤í¬ ìˆ˜ (ê¸°ë³¸: ì „ì²´)')
    parser.add_argument('--quick', action='store_true',
                        help='ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (ê° ë ˆë²¨ë‹¹ 1ê°œ)')
    parser.add_argument('--log-file', help='íŠ¹ì • ë¡œê·¸ íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)')

    # ì¶œë ¥ ì„¤ì •
    parser.add_argument('--output', default='reports',
                        help='ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: reports)')
    parser.add_argument('--format', nargs='+',
                        default=['json', 'csv', 'markdown'],
                        choices=['json', 'csv', 'markdown', 'all'],
                        help='ì¶œë ¥ í˜•ì‹ (ê¸°ë³¸: json csv markdown)')
    parser.add_argument('--verbose', action='store_true',
                        help='ìƒì„¸ ë¡œê·¸ ì¶œë ¥')

    args = parser.parse_args()

    # Judge ëª¨ë¸ ì„¤ì •
    if args.judge_models:
        judge_models = args.judge_models
    else:
        # ê¸°ë³¸ê°’: ë‹¨ì¼ Judge 
        judge_models = ["azure/gpt-4o"]

    # ë ˆë²¨ íŒŒì‹±
    levels = None
    if args.levels:
        # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë ˆë²¨ íŒŒì‹± (ì˜ˆ: "L1,L3,L5" ë˜ëŠ” "1,3,5")
        levels = [lvl.strip() for lvl in args.levels.split(',')]
        print(f"[ì„¤ì •] í‰ê°€ ëŒ€ìƒ ë ˆë²¨: {', '.join(levels)}")

    # í‰ê°€ ì‹¤í–‰
    try:
        evaluator = ModelRunEvaluator(
            date=args.date,
            model=args.model,
            judge_models=judge_models,
            sample_size=1 if args.quick else args.sample,
            levels=levels,
            verbose=args.verbose
        )

        report = evaluator.evaluate()
        evaluator.export(report, args.output, formats=args.format)

        print("\n[ì™„ë£Œ] í‰ê°€ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n")

    except Exception as e:
        print(f"\n[ì˜¤ë¥˜] í‰ê°€ ì‹¤íŒ¨: {e}\n")
        import traceback
        traceback.print_exc()
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
